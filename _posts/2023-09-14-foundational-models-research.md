---
layout: post
title: list of foundational models 
date: 2023-09-14
categories: [research]
tags: [cli, research]
description: research on various foundational models
author: Sunil Dhaka
---
| Name                                                        | Cost                                  | Arch                                                      | Link                                                                                                    | Licence | Input           | Output | Remark                                                                                                                                                                                |
|-------------------------------------------------------------|---------------------------------------|-----------------------------------------------------------|---------------------------------------------------------------------------------------------------------|---------|-----------------|--------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Vid2Seq: Visual Language Model for Dense Video Captioning   | 500M parameters (200M text, 300M video) | Page 3 | [Paper](https://arxiv.org/pdf/2302.14115.pdf)                   | Open    | Text + Video    | Text   | Since it was trained on an unlabeled narrated dataset, we can fine-tune it to our dataset (automated curation possibility needs to be checked) with transfer learning.      |
| OpenFlamingo: Vision Language Model | 3B to 9B parameters variants         | [Flamingo Arch, DeepMind](https://github.com/mlfoundations/open_flamingo/blob/main/docs/flamingo.png) | [Paper](https://arxiv.org/pdf/2308.01390.pdf) | Open    | Image-Text      | Text   | This is an open-sourced model that reproduces Google's Flamingo, which was closed. It is also dependent on CLIP and LLaMA.                                                        |
| LLaMA: Open and Efficient Foundation Language Models        | 7B to 65B parameters variants | [Open Source Research from Meta](https://ai.meta.com/blog/large-language-model-llama-meta-ai/) | [paper](https://arxiv.org/pdf/2302.13971.pdf)  | Open    | Text            | Text   | It is an open-sourced LLM from Meta that competes with other closed or limited LLMs from other organizations.                                                                     |
| InternVideo: General Video Foundation Models via Generative and Discriminative Learning | 1.3B parameters | Fig 2| [Paper](https://arxiv.org/pdf/2212.03191.pdf) | [Open](https://github.com/OpenGVLab/InternVideo) | Text-Video | Various vision-related tasks (described in their Git repo) | For training, it is using many open-source video-text datasets.                        |
| InternImage: Exploring Large-Scale Vision Foundation Models  | Unknown                               | Fig 3                                                      | [Paper](https://arxiv.org/pdf/2211.05778.pdf) | [Open](https://github.com/OpenGVLab/InternImage) | Image-Text | General image-related tasks | Open-source model from OpenGVLab.    |
| Make-A-Video: Text-to-Video Generation without Text-Video Data | Unknown                         | Fig 2                                                      | [Paper](https://arxiv.org/pdf/2209.14792.pdf) | Closed  | Text-Image / Video | Video Generation / Image Animation / Video Variation | Their specifically curated dataset is also limited for use only.    |
| CogVideo: Transformer Model for Text-to-Video Generation     | 9.4 billion parameters, trained on 5.4 million text-video pairs | Fig 2 | [Paper](https://arxiv.org/pdf/2205.15868.pdf) | [Open](https://github.com/THUDM/CogVideo) | Text-Video | Video | Demos of the model are available on [Hugging Face](https://huggingface.co/spaces/THUDM/CogVideo).    |
| CLAP                                                        | Unknown                               | [Arch](https://raw.githubusercontent.com/LAION-AI/CLAP/main/assets/audioclip-arch.png)  | [Paper](https://arxiv.org/pdf/2211.06687.pdf) | [Open](https://github.com/LAION-AI/CLAP) | Audio-Text | Audio  | -                                                         
| VLMo: a model for text-to-image generation              | 512M parameters  | Fig 1                                                  | [Paper](https://arxiv.org/pdf/2111.02358.pdf)   | [Closed](https://github.com/microsoft/unilm/tree/master/vlmo) | Text-Image      | Vision-Language Tasks     | Developed by Microsoft. Code repo under Microsoft License.                                                |
| CogView: a transformer model for text-to-image generation | 4B parameters    | Fig 3                                                  | [Paper](https://arxiv.org/pdf/2105.13290.pdf)   | [Open](https://github.com/THUDM/CogView)                         | Text-Image      | Image                     | Open-source model from THUDM.                                                                               |
| Jukebox: A Generative Model for Music                    | 5B parameters    | Fig 8                                                  | [Paper](https://arxiv.org/pdf/2005.00341.pdf)   | [Open](https://github.com/openai/jukebox)                      | Audio           | Audio                     | Open-source project from OpenAI.                                                                            |
| Gorilla: Large Language Model Connected with Massive APIs | 7B parameters    | Fig 3                                                  | [Paper](https://arxiv.org/pdf/2305.15334v1.pdf) | [Open](https://gorilla.cs.berkeley.edu/)                        | Text            | API Call Code             | Fine-tuned LLaMA-based model performing better than GPT-4 on writing API calls from natural language input. |
| Voicebox: Text-Guided Multilingual Universal Speech Generation at Scale | 330M parameters | Not in the paper                                       | [Paper](https://scontent-tir3-2.xx.fbcdn.net/v/t39.8562-6/354636794_599417672291955_3799385851435258804_n.pdf?_nc_cat=101&ccb=1-7&_nc_sid=ad8a9d&_nc_ohc=ldtnb4o-69AAX8_IUAt&_nc_ht=scontent-tir3-2.xx&oh=00_AfBUFRGmCVWKhX-V6NrGoGBBVhnNqan4rbE4-veqZIpolQ&oe=65072E71) | [Closed](https://research.facebook.com/publications/voicebox-text-guided-multilingual-universal-speech-generation-at-scale/) | Text-Audio | Audio                     | First generative AI model for speech to generalize across tasks with SOTA from META.                    |
| Otter: A Multi-Modal Model with In-Context Instruction Tuning | 1.3B parameters  | [Arch](https://github.com/Luodian/Otter)                  | [Paper](https://arxiv.org/pdf/2305.03726v1.pdf) | open                                               | Text-Image      | Text                      | Multi-modal model based on OpenFlamingo.                                                                  |
